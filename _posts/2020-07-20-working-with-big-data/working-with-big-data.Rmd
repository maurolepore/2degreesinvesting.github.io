---
title: "Working with big data"
description: |
  A short description of the post.
author:
  - name: Mauro Lepore
    url: https://github.com/maurolepore
date: 07-20-2020
output: github_document
  # distill::distill_article:
  # toc: true
  # toc_depth: 3
  # self_contained: true
categories:
  - r2dii
  - package
preview: preview.jpg
twitter:
  site: "@mauro_lepore"
  creator: "@mauro_lepore"
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "#>",
  collapse = TRUE,
  # Benchmark each run
  cache = TRUE
)
```

```{r}
# Packages
library(tidyverse)
library(fs)
library(vroom)
library(glue)
library(bench)
library(r2dii.data)
library(r2dii.match)
packageVersion("r2dii.match")

# Example datasets
lbk_full <- loanbook_demo
ald_full <- ald_demo
```

## How do you eat an elephant?

One way to save time and memory is to use less data. Even if you downsize your data, you may achieve the exact same result, or achieve a slightly different result that is equally informative.

### Use just the columns you need

Your loanbook dataset may be unnecessarily big; it may have columns that `match_name()` doesn't use but make it less efficient. If you feed `match_name()` with only the crucial columns it needs, you may save time and memory.

```{r}
dim(lbk_full)

lbk_crucial_cols <- lbk_full %>% select(crucial_lbk())
dim(lbk_crucial_cols)
```

Compare:

```{r, cache=TRUE}
benchmark <- bench::mark(
  check = FALSE,
  iterations = 20,
  lbk_full = match_name(lbk_full, ald_demo),
  lbk_crucial_cols  = match_name(lbk_crucial_cols, ald_demo)
)

ggplot2::autoplot(benchmark)
```

The difference here is small, but can increase with the size of the data.

## Chunk your data

Before you saw that one way to save time and memory is to use fewer columns of the loanbook dataset. And you can work yet more efficiently if you use fewer rows of the ald dataset. One way is to focus on a single sector.

```{r}
dim(ald_full)

ald_one_sector <- filter(ald_full, sector == "power")
dim(ald_one_sector)
```

Compared to using the full datasets, this should use less time and memory.

```{r}
benchmark <- bench::mark(
  check = FALSE,
  iterations = 30,
  full = match_name(lbk_full, ald_full),
  crucial_cols_one_sector = match_name(lbk_crucial_cols, ald_one_sector)
)

ggplot2::autoplot(benchmark)
```

To study multiple sectors you can process each one at a time. Each result may be large, and storing it in memory may cause your computer to crash. Instead, you can save the output of each sector to a file.

```{r}
# Create a directory to store the output
directory <- "output"
if (!dir_exists(directory)) dir_create(directory)

ald_power <- filter(ald_full, sector == "power")
matched_power <- match_name(lbk_crucial_cols, ald_power)
vroom::vroom_write(matched_power, path = "output/power.csv")

ald_aviation <- filter(ald_full, sector == "aviation")
matched_power <- match_name(lbk_crucial_cols, ald_aviation)
vroom::vroom_write(matched_power, path = "output/aviation.csv")

# See the output files we saved
all_sectors <- fs::dir_ls(directory)
all_sectors
```

When you are ready, combine all results and continue the analysis.

```{r}
matched <- vroom::vroom(all_sectors)
matched

# How many matches per sector?
count(matched, sector)
```

