---
title: "Working with big data"
description: |
  A short description of the post.
author:
  - name: Mauro Lepore
    url: https://github.com/maurolepore
date: 07-20-2020
output: github_document
  # distill::distill_article:
  # toc: true
  # toc_depth: 3
  # self_contained: true
categories:
  - r2dii
  - package
preview: preview.jpg
twitter:
  site: "@mauro_lepore"
  creator: "@mauro_lepore"
editor_options: 
  chunk_output_type: console
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "#>",
  collapse = TRUE,
  cache = FALSE
)
```

```{r}
# Packages
library(tidyverse)
library(fs)
library(vroom)
library(bench)
library(ggplot2)
library(r2dii.data)
library(r2dii.match)
packageVersion("r2dii.match")

# Example datasets
lbk_full <- loanbook_demo
ald_full <- ald_demo
```

## How do you eat an elephant?

One way to save time and memory is to use less data. Even if you downsize your data, you may achieve the exact same result, or achieve a slightly different result that is equally informative.

### Use just the columns you need

Your loanbook dataset may be unnecessarily big; it may have columns that `match_name()` doesn't use but make it less efficient. If you feed `match_name()` with only the crucial columns it needs, you may save time and memory.

```{r}
dim(lbk_full)

lbk_smaller <- lbk_full %>% select(crucial_lbk())
dim(lbk_smaller)
```

Compare:

```{r, cache=TRUE}
benchmark <- bench::mark(
  check = FALSE,
  # iterations = 30,
  bigger = match_name(lbk_full, ald_demo),
  smaller  = match_name(lbk_smaller, ald_demo)
)

benchmark %>% autoplot()
```

The difference here is small, but can increase with the size of the data.

## Chunk your data

Before you saw that one way to save time and memory is to use fewer columns of the loanbook dataset. And you can work yet more efficiently if you use fewer rows of the ald dataset. One way is to focus on a single sector.

```{r}
ald_full %>% dim()
ald_full %>% filter(sector == "power") %>% dim()
```

Compared to using the full datasets, this should use less time and memory.

```{r cache=TRUE}
benchmark <- bench::mark(
  check = FALSE,
  # iterations = 30,
  bigger = match_name(lbk_full, ald_full),
  smaller = match_name(lbk_smaller, filter(ald_full, sector == "power"))
)

benchmark %>% autoplot()
```

To study multiple sectors you can process each one at a time. Each result may be large, and storing it in memory may cause your computer to crash. Instead, you can save the output of each sector to a file.

```{r}
# Create a directory to store the output
if (!dir_exists("sectors")) dir_create("sectors")

power <- match_name(lbk_smaller, filter(ald_full, sector == "power"))
power %>% vroom_write(path("sectors", "power.csv"))

aviation <- match_name(lbk_smaller, filter(ald_full, sector == "aviation"))
aviation %>% vroom_write(path("sectors", "aviation.csv"))

dir_ls("sectors")
```

When you are ready, combine all results and continue the analysis.

```{r}
sectors <- dir_ls("sectors") %>% vroom()
sectors
```

Let's see how many matches we got per sector:

```{r}
sectors %>% count(sector)
```

Cleanup

```{r}
dir_ls("sectors") %>% file_delete()
```

## Slice loanbook by row

What if your dataset is so large than even one sector is too big? Or what if you want to try matches across sectors?

You can slice each row of the `loanboook` dataset and match it against the entire `ald` dataset; then if that row matched nothing, discard it, else save the result to a file. Before you try this approach, beware it can be painfully slow; but it should work even if you have little memory.

```{r cache=TRUE, warning=FALSE, message=FALSE}
if (!dir_exists("rowwise")) dir_create("rowwise")

loanbook <- head(lbk_smaller, 20)
ald <- ald_full
for (i in 1:nrow(loanbook)) {
  out <- match_name(slice(loanbook, i), ald)
  if (nrow(out) == 0L) next()
  out %>% vroom_write(path("rowwise", paste0(i, ".csv")))
}
```

The output directory now contains one file per matching row.

```{r}
dir_ls("rowwise") %>% length()
```

But we can treat it as a single file because `vroom()` can read them all at once and produce a single data frame.

```{r}
rowwise <- dir_ls("rowwise") %>% vroom()
rowwise

rowwise %>% count(sector)
```

Cleanup

```{r}
dir_ls("rowwise") %>% file_delete()
```

## Arbitrary "chunks" of loanbook data

Feeding `match_name()` with individual can be too slow. You can feed `match_name()` with "chunks" of your `loanbook` dataset that are bigger than a single row, yet small enough you can process each chunk with whatever memory you have.

Let's create two helper functions: `chunkid()` to identify all rows in a loanbook that belong to the same chunk, and `vroom_chunks()` to write a .csv file with the results of matching each `loanbook` chunk agains the entire `ald` dataset.

```{r}
chunkid <- function(n) as.integer(cut(row_number(), breaks = n))

vroom_chunks <- function(path, loanbook, ald, ...) {
  stopifnot(hasName(loanbook, "chunkid"))
  
  if (!dir_exists(path)) dir_create(path)
  
  for (i in unique(loanbook$chunkid)) {
    matched <- match_name(filter(loanbook, chunkid == i), ald, ...)
    
    if (nrow(matched) == 0L) next()
    matched %>% vroom_write(path(path, paste0(i, ".csv")))
  }
  
  invisible(path)
}
```

With increasing number of chunks, processing time increases but memory decreases. You should vary this parameter to find what's best for you.

```{r}
lbk <- head(lbk_smaller, 50)

benchmark <- bench::mark(
  check = FALSE,
  n3  = path_n3 <- path(path_temp(), "n3") %>% 
    vroom_chunks(mutate(lbk, chunkid = chunkid(3)), ald_full),
  n30 = path_n30 <- path(path_temp(), "n30") %>% 
    vroom_chunks(mutate(lbk, chunkid = chunkid(30)), ald_full)
)

benchmark %>% autoplot()
```

```{r}
dir_ls(path_n3)

chunks <- dir_ls(path_n3) %>% vroom()
chunks %>% nest_by(sector)

chunks
```

Cleanup

```{r}
dir_ls(path_n3) %>% file_delete()
dir_ls(path_n33) %>% file_delete()
```

## Pick the most important loans

Another option is to feed `match_name()` with data of only the loans that make up most of the credit limit or outstanding credit limit, for example, you may use only the largest loans that represent 80% of the credit.

Let's glimpse the columns that contain the pattern "loan_size":

```{r}
lbk_full %>% 
  select(contains("loan_size")) %>% 
  glimpse()
```

The `loan_size_*` values are comparable across rows because they are all expressed in EURO:

```{r}
lbk_full %>% 
  distinct(loan_size_outstanding_currency, loan_size_credit_limit_currency)
```

And the values in each row correspond to a unique loan:

```{r}
nrow(lbk_full)
nrow(distinct(lbk_full, id_loan))
```

We can now arrange the data in descending order of the `loan_size_*` columns, calculate the cumulative percent for each of them; and pick the top loans that make up to 80% of the credit:

```{r}
percent <- function(x) x / sum(x) * 100

top80 <- lbk_full %>% 
  arrange(desc(loan_size_credit_limit), desc(loan_size_outstanding)) %>% 
  mutate(
    cum_credit_limit = cumsum(percent(loan_size_credit_limit)),
    cum_outstanding  = cumsum(percent(loan_size_outstanding))
  ) %>% 
  filter(cum_credit_limit <= 80, cum_outstanding <= 80)

top80 %>% 
  select(id_loan, starts_with("cum_"), everything())
```

The result is a dataset with considerably fewer rows that should use less time and memory while capturing the main pattern.

```{r}
round(nrow(top80) / nrow(lbk_full) * 100)
```

```{r}
b <- bench::mark(
  check = FALSE,
  iterations = 30,
  all_loans = match_name(lbk_smaller, ald_demo),
  top80 = match_name(select(top80, crucial_lbk()), ald_demo)
)

autoplot(b)
```

